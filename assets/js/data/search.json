[ { "title": "Faster Page Loads - Reducing the Time to First Byte (TTFB) by 50%", "url": "/posts/optimizing-ttfb/", "categories": "", "tags": "Software Engineering, AWS, Angular, Performance, Cloud, SPA", "date": "2024-02-08 11:59:17 +0500", "snippet": "Oct, 2023: Engineering at Rewaa decided to improve the page load performance of its Platform Webapp. Of the many performance metrics available to us, we prioritized improving Time to First Byte (TTFB). TTFB measures the duration from the user or client making an HTTP request to the first byte of the page being received by the client’s browser. – WikipediaWhy TTFB?One might argue, why optimize TTFB ahead of user centric metrics like LCP and FCP? To answer this, we have to understand the significance of optimizing TTFB when serving Single Page Applications (SPAs).A Single Page Application (SPA) has to deliver the index.html file to the browser first. The index.html file then requests for more files (css,js,fonts) which execute and render meaningful content for the user. The sooner the index.html is delivered to the browser, the quicker user centric content can start rendering. This is why TTFB is crucial when serving SPAs.With the case established for optimizing TTFB, we decided to punch above our waist and set our target to reduce TTFB to less than 500ms.What is a good TTFB score?The quicker, the better. However it is understood that anything below 800ms is good.Measuring TTFB“You cannot improve what you don’t measure”.You can measure TTFB by: Network Tab of the browser Performance analysis through Lighthouse (or a similar tool)It is important to understand that TTFB depends on many factors resulting in different measures for each user. Measuring a 75th percentile of Average TTFB across all users was more meaningful to track as compared to single user values provided by the options mentioned above. I was able to setup the graph using Real User Monitoring (RUM) on Datadog.The numbers proved we were not Poor but not Good either. … we decided to punch above our waist and set our target to reduce TTFB to less than 500ms.Current Content Delivery SetupOur Platform Webapp is a Single Page Application built with Angular, being served from Mumbai through AWS CloudFront. SPAs require a specific kind of routing where each registered route within the application should return the index.html file. Our current setup on CloudFront was doing that but in a strange way. Each registered route resulted in a Http 404, which we intercepted and returned the index.html file along with it.Limitations of the current Content Delivery setupI realized there are several problems with our setup: All initial page loads were 404s with index.html: It works but it has its side effects which maybe unknown. One such example is, running “Lighthouse” page load performance refuses to compute results because index.html has a 404 - Not Found status. All requests were resulting in a cache miss: Each cache miss resulted in content being delivered directly from the origin. No advantage was being taken from the intermediate caches. Mumbai (our primary region) was serving the entire world irrespective of distance. SolutionsThe lifecycle events illustrated below forms the foundations of all the performance gains, so pay close attention to it.Routing fixThe first step was to get rid of the 404s by returning the index.html file on all routes without an extension (png,css,woff). Lambda@Edge, which is a specific kind of lambda function has the ability to execute code before the request reaches CloudFront.The ability to intercept a request before it reaches CloudFront Cache i.e. Viewer Request is all I needed. In the following few lines, I was able to rewrite the incoming requests to index.html.export const handler = async (event, context, callback) =&gt; { const { request } = event.Records[0].cf; const re = /(?:\\.([^.]+))?$/; if (!re.exec(request.uri)[1]) { const newuri = request.uri.replace(/.*/, '/index.html'); request.uri = newuri; } callback(null, request);};With Lambda@Edge setup to execute at Viewer Request, I just removed the Custom error response from Error pages to rid the setup for 404 intercepts.Caching fixThe next target was to minimize the cache miss illustrated in the diagrams above. To cache content, the CDN looks for the Cache-Control headers. The Cache-Control header in our app was set to no-cache, no-store, must-revalidate. Let’s break this down: no-cache: This does not mean “don’t cache”. Cache will always revalidate content from the source before serving. no-store: No content would be stored in any cache. must-revalidate: Response will be reused while fresh. Response will be validated once stale.The no-store not allowing anything to be stored made the no-cache and must-revalidate directives practically useless. Worth mentioning that must-revalidate always should be used with max-age to help the cache identify fresh content. In short, the configuration was completely broken.By setting Cache-Control header to must-revalidate max-age=604800, CloudFront served the cached responses until a reasonable amount of time (7 days) has passed or there is new content available. Setting the correct cache headers also enabled ETag on each request resulting in a Http 304 Not Modified if the content did not change. For new releases, we programmatically invalidated all CloudFront caches to ensure the new release always results in fresh content for our users.Icing on the cake - CloudFront FunctionsWhile monitoring the logs I noticed that all content is being served from Frankfurt. With most of our customer base in Saudi Arabia, not serving content from Mumbai (our deployment region) was definitely a win. I wondered if there were still a few more millisecond I could reduce by serving from an Edge Location instead of a Regional Edge Cache. tl;dr Edge Location serves content with reduced latency because it is nearer to the users as compared to the Regional Edge Cache.The reason why all our content was being served from Frankfurt is because Lambda@Edge executes only on the Regional Edge Cache. Since our routing logic was simple, moving it to CloudFront Functions (Event source: Viewer Request) was the better choice. A clear reduction of approx. 100ms was noticed by moving from Lambda@Edge to CloudFront Functions You can find all the difference between Lambda@Edge and CloudFront Functions at this link.Bonus Discovery along the wayLighthouse also indicated that served content is uncompressed resulting in high volumes of data transfer. Enabling GZip compression can reduce the data transfer volume up by 75%. Enabling GZip only required setting the CacheOptimized policy under Behaviors of the CloudFront distribution.ResultsI recorded the TTFB numbers from 4PM to 9AM (the next day) for 3 consecutive weekdays to test the hypothesis. Here are the results: Function Avg. TTFB (75pc) No function 987.45ms Lambda@Edge 490.34ms CloudFront Functions 401.46ms And here are the numbers for the amount of data transferred for 2 consecutive months: Dates Data Transferred 25th Oct - 24th Nov 9.49 GB 25th Nov - 25th Dec 2.02 GB And our final content delivery setup became:Overall, the results came out to be just fantastic. The 75pc of TTFB was reduced by 50% and the Total Data Transfer was reduced by 75% on average. Target of reducing TTFB to less than 500ms was successfully achieved." }, { "title": "Building a notification system over a webapp", "url": "/posts/building-a-notification-system-over-a-webapp/", "categories": "", "tags": "Software Engineering, AWS, NodeJS", "date": "2022-12-25 04:59:17 +0500", "snippet": "Up until the world exploded into apps, communication between the schools and children’s parents was done through the children’s diary. Fast forward today, diaries are no longer in fashion. Every school has an app to “keep you updated”. The problem with apps is that it should have an amazing notification system. It turns out, my son’s school did have an app but did not have a functional notification system. My son kept missing school events and that’s when I decided to tinker around to see if I could make a notification system on top of the schools apps.Bird’s eye viewThe school has a web app which meant that having the right credentials, the information is accessible over the internet. A quick skim through the “Network Tab” in the browser dev-tools gave me the HTTP call which returned an HTML document with the required data. All I wanted now was to parse the HTML, extract what I wanted from the DOM and send a notification if there is something “new”.I decided that notification should be an SMS to avoid complicating the system. The content of the SMS can be kept simple, e.g. “New notification on ABC School Portal. Please login to https://parent.abc.edu”.Running the above cycle few times a day means we’d have to be super lazy to miss them. Next step, Code!Engineering Details:To decide on how to build this, I asked myself the following questions: How long will each execution take to run? - few seconds How often will we need to schedule executions? - every 6 hours Does the application need any datastore or state? - noAnswers above, coupled with no desire to manage infrastructure, serverless on AWS became an obvious choice. A Lambda function, scheduled to run through CloudWatch Events and sending an SMS through SNS should suffice.I will not be diving into the how to’s as there is a lot of content available on the internet on setting up Lambda functions, CloudWatch alarms and sending SMS via SNS.Keeping it under free-tier:It makes no sense to spend money on a feature the school engineering team should have provided out of the box. Free tier limits for AWS Lambda, SNS and CloudWatch are very gracious for a use case like this. The only challenge for me was the SMS which are never free. AWS SNS sandbox provides upto 10 numbers on which SMS-es can be sent without incurring any cost. This was more than enough for me. I added a couple of phone numbers and started receiving SMS-es at no-cost.Final outlook:The notification system is definitely helping us stay on top of the school events. A similar notification system can be build upon practically any web application provided you stay within their server constraints.The code can be found here. One fine day, I will move this to AWS CDK." }, { "title": "Adieu Airlift", "url": "/posts/adieu-airlift/", "categories": "", "tags": "Software Engineering Management, Thoughts", "date": "2022-07-27 04:59:17 +0500", "snippet": "The dreaded second week of July, 2022. Airlift decides to close all of its operations in Pakistan. It was good while it lasted.I have been in the tech space of Pakistan since 2012. A tech space dominated by companies that run on a proven business model of outsourcing. For outsourcing, it is kindergarten math. Sell for $100, spend $40, remaining in the bank, rinse and repeat. From the run-of-the-mill in the past to testing deep, uncharted waters in Airlift, I had discovered myself as a completely different individual in 13 months. I will forever be thankful to the teammates at Airlift for challenging me and bringing out the best in me.Going back to 2020, covid hit, travel got cheap and I landed in Istanbul for a good month in Dec 2020. This is where it all started.PrologueJan 2021, I was visiting my sister’s in Istanbul, Turkey. I quickly realized that she did not step out of her home to get groceries. This was my first exposure to quick-commerce. A few taps, and virtually anything landed at the doorstep within 60 mins. With the impact and the power felt, I realized there is so much we can do to improve lives with technology. My contributions to client/enterprise software quickly felt unfulfilling. I wanted to touch lives through my work. The decision had been made.Enter Airlift:I landed back in Karachi after a month. Naturally there was no grocery at home. We couldn’t find a place to shop at 4 in the morning. On the 17th of Jan, 2021, I downloaded Airlift and received my first order, right at my doorstep. This is where it hit me. I wanted to join Airlift. I wanted to experience it. A month later, I was interviewing at Airlift. May, 30th, 2021 was my first day as Engineering Manager at Airlift.“I believe I can fly”Getting into Airlift was just the tip of the iceberg but for once I felt “sky’s the limit”. In a few months, an engineer who couldn’t even decide his next project was challenging a product with feature requisitions, re-organizing teams for throughput, hiring talent and building software for scale. The aura made me feel powerful. I quickly realized that I had spread my wings and was ready to fly.The LandingLaunching 2 countries, completing 30,000+ orders per day, it was a joy ride. As engineers, we felt pride in what we achieved through our work. We had become battle tested, ready to take on anything and everything. I term the closure of Airlift as a “Landing” because when the time is right, the “landed” will take off again.EpilogueTo the people on the shore, you’d always see that the ship had sunk but what you don’t see is that each member of the crew walked out as a Captain. Airlift has transformed me into an individual who is ready to challenge the status quo. Airlift taught me to be fearless in front of failure. Airlift taught me to believe in myself and for that I will forever be grateful. Airlift is not just a product, it’s a mindset that I will carry wherever I go.Adieu Airlift." }, { "title": "Engineering Management in Software", "url": "/posts/engineering-management-in-software/", "categories": "", "tags": "Software Engineering Management", "date": "2022-05-13 04:59:17 +0500", "snippet": "On May 30th, 2021, after spending almost 8 years in an individual contributor role, I stepped into the uncharted waters known as “Engineering Management”. Having spent almost a year as an EM, I’ve realised that it’s a complex mix of engineering and management. Let’s zoom in.Why do we need an Engineering Manager (EM) on the team?Software products are an intersection between the business and engineering functions within a company. To lead a software product (or a part of it), team needs a leader who not only knows the business but can also engineer the product behind the business. Engineering managers are the people who interface between engineering and business and naturally are critical to the success of a software product. Having a leader who is missing on either business or the engineering capabilities can prove to be a matter of success or failure for the software product.What does an Engineering Manager do?An Engineering Manager is the bridge between the business and the engineering teams. An EM has the following responsibilities:Contribute in technologyEMs are engineers at heart. They do write some code to keep their skills sharp but generally spend time in building engineering processes such as technical design/code reviews and code coverage to ensure the correct code ends up in the commits.Along with being the flag bearer of code quality, an EM should also be a great thought partner in technical discussions with senior engineers and architects to help them make decisions in the light of business needs and goals.Manage DeliverablesThe EM lays out the plan, commits the timelines, builds processes that ensure the timelines and business requirements are met, and keeps a health check on deliverables once shipped. To achieve this, an EM makes use of the right tools for roadmap planning, execution, documentation, and checks the pulse via daily stand-ups, feature design reviews, testing and post-delivery feedback.Build the teamTeam building has two aspects a) growing existing talent and, b) hiring talent. Neither takes precedence over the other.EMs should be hosts of exceptional interpersonal skills, allowing them to support teammates towards their aspirations and suitable career paths. Scheduled 1-1s allow EMs to build a direct connection with their teammate, allowing them to empathize, provide candid feedback, and set goals.There will be times when a team requires more firepower – therefore, an EM is also responsible for hiring the right talent. The right talent means that the shortlisted candidate brings compatible strengths and the right attitude. EMs should enable themselves by collecting sufficient data throughout the process to help them make an informed hiring decision, backed by conviction.Carry the right business acumenBuilding software without understanding the why will lead to engineering decisions that may not be in line with business goals. This is where EMs play the role of an interface between the business and the engineering teams. Having a deep understanding of both the business goals and the engineering goals, EMs ensure the right engineering choices are being made to support the business goals. Engineers who are able to connect their work to the bigger picture are more excited and engaged in building the product and EMs are the people who make this happen.Champion and evangelize culture &amp; valuesBeing the captain of the ship, an EM should build a strong culture of ownership, empathy, openness, humility, and respect within the team. Along with the constant preaching across various platforms, an EM has to lead by example on the culture and values of the organization and build the same across their team.To summarise, an Engineering Manager is the essential cog between Engineering and Business which makes the clock work." }, { "title": "The art of conducting meetings", "url": "/posts/the-art-of-conducting-meetings/", "categories": "", "tags": "Software Engineering Management", "date": "2021-09-09 04:59:17 +0500", "snippet": "If it is 7 PM and you think you have spent your day in meetings without accomplishing much, there is a high chance you need to recalibrate. Too many meetings in a day take up too much time whereas too little may mean you are missing out on important decisions during the day. Meetings, if not done right, will become a manager’s bane.Over the years, working with managers which are great and terrible at meetings, I have devised a framework on conducting meetings which prompt action and drive decisions. This is my first effort to pen this down, so here goes.Define the agenda of the meeting:Everyone has received a meeting invite without an agenda. A person sending a meeting invite without a defined agenda is as blank as the people who are going to attend it. Writing down the agenda in the calendar invite has the following benefits: It allows the sender to rethink – do we really need this meeting? If so, why exactly? It enables the sender to plan the meeting with pointers to cover, automatically directing the focus of the conversation It gives a clear picture to the attendees on what to expect from the meeting and how they can come prepared to itWhenever you send out a meeting invite, outline a brief agenda you would want to cover – it will smoothen the meeting execution tenfold.Define the outcome of the meeting:The agenda of the meeting should always contain the field “desired outcome”. Defining the “desired outcome” will focus everyone’s energies on solving the problem at hand. Once the intent is there, results will naturally follow. Naming this section as “desired outcome” is done to add a touch of pragmatism – there will be times when the outcome you desired will not be the outcome of the meeting and that is completely okay.Identify the attendees:No one wants to be in a meeting where they feel uninvited. Attendees should always be chosen to attend carefully. We can divide all attendees into the following roles: Anchors: You should always have an anchor to run the meeting. It is often the organizer but can be anyone else as well. The best organizers are those who let people know who is going to anchor the meeting at least a couple of hours before the meeting. Not having an anchor is like not having Google Maps – you will get there but will make wrong turns, wasting people’s time in the process. Decision-makers: They are key to the outcome of the meeting. Cancel if the decision maker has declined the meeting or reschedule if s/he is a no show. In every meeting there should be at least one decision-maker. In case there are more than one decision makers, the anchor should focus on building consensus if there is a difference of opinion. Contributors: It is good to have them. They should help understand the implications of decisions by presenting different viewpoints. If a contributor is essential to the decision due to some knowledge s/he has, reschedule if they decline or are a no show. Observers: It is beneficial for everyone to leave the observers out. If they just need to listen, just include them in the outcome email. Put a timer:Digital calendars today have made it mandatory to time bound meetings but if that doesn’t work for you, please don’t buy a premium Zoom account. Allow it to kick you out exactly after 40 mins. Managing time is the responsibility of the anchors of the meeting. Anchors should focus on the following points during the meeting: Anchors should spend a maximum of 2-3 mins to set the theme of the meeting by going over the agenda and the desired outcomes Anchors should ensure each agenda item gets its fair share of time from the meeting. If the anchor feels someone is unwilling to lose the spotlight, s/he should make a move to let the person know they are taking too much time Anchors should keep an eye on the agenda items left to discuss and the remaining time in the me eting. If the anchor feels that the meeting is going off track due to 3) or any other reason, s/he should make a conscious effort to bring the meeting back on track There will be times when the best of anchorage will still not come to a desired outcome from the meeting. In such cases, it would be best to reschedule a meeting with all the above points in consideration.Publish the outcomes:Publishing the outcomes of a meeting in any form carries immense value. Let’s list a few down: People who just need to be informed of the outcome of the meeting can just go through the text in a few mins instead of spending an hour in the meeting. Yes, observers, that is you! Outcomes clearly define the action items or next steps from a meeting. These should always be assigned and have a deadline and follow up date letting everyone know what’s expected of them and when. Define the need for a follow up meeting. In this case do not forget to follow all the above. Hope this helps." }, { "title": "How we messed up our Angular App", "url": "/posts/how-we-messed-up-our-angular-app/", "categories": "", "tags": "AngularJS", "date": "2015-03-15 04:59:17 +0500", "snippet": "Back in July of 2013, it happened and we were writing an Angular app. Working on Angular JS for the first time, we made some choices which are worthy of entering the Hall of Shame.Inspired by Dave Smiths talk (by the same title as this post) at the NG-NL Conf, I want to share exactly how “we” messed up our Angular app so you don’t.1. Using Yeoman’s Generator Angular:What we did:The team decided to use a Yeoman Generator with the most number of downloads. The clear winner was generator-angular. A year into development with generator-angular, we realized that the file and folder structure offered by this generator does not scale. The application was fairly large and it felt like a mess. With all the development done, we could not go down the refactor lane and the application still feels like a Ferrari engine plugged into Fred Flintstone’s car.What we should have done:A great way to structure medium to large scale applications is breaking them into features and modules. It gives you the following benefits: Makes it easier to find code and maintain Encourages code reusibility2. Abusing the $rootScopeWhat we did:Whenever we had to put some global state variables like some messages or logged in user information, $rootScope was our best friend unless we found out its true colors. From there on, things became insanely difficult to maintain. Remember, anything on the $rootScope is prototypaly inherited into each $scope across the application and as a result of this, we had no clue who is changing what.What we should have done:Services are they way to go here. We should have exposed appropriate methods to get or set the data we need. This is a more structural and reliable way of approaching the problem of global application states. Also, there would be no variable shadowing on the inherited scopes from the $rootScope.3. Bloated $scope inside controllersWhat we did:With all the magic we found $scope does, the adrenaline rush made us put private functions, private variables, reuseable behavior on the $scope. Problem with putting everything on the $scope is that it bloates it creating a $scope soup.What we should have done:We were late to understand the purpose of the controllers and the $scope. The things which need to be bound on the view should be on the $scope, rest should either be private variables or in terms of reuseable logic, it should be in services.4. Forcing controller inheritanceWhat we did:I found that there is a very strange way to inherit controller behavior in one of our files. I still fear to refactor and fix it as the application is live.What we should have done:If there is any reuseable behavior we need, it should be in the services, I dont feel there is a need to explain this anymore.5. Not namespacing directivesWhat we did:Directives are a world of their own. There are a tons of things you can do with them but things as subtle as their name can cause lots of readability and debugging issues. We could not differentiate between native HTML5 tags and custom directives.What we should have done:Prefixing directives with a two letter acronym of your application eases this. All we needed was name field directive as fpField. Then, it can be used as fp-field on the markup and at a glance the developer can recognize that this is a custom directive.6. Not using HTTP InterceptorsWhat we did:To handle error responses, we made a function on a service and passed that function to the error handler of every $http request. This violates DRY and was a nuisance to add with every request. We could have wrapped $http in a service and chained the error handler there, but thats not pretty either.What we should have done:We just needed to use an $http error interceptor and pass the same function to it. It is more centralized and maintainable approach to error handling $http requests.How did you mess things up?" }, { "title": "Generator ng-Super Philosophies", "url": "/posts/generator-ng-super-philosophy/", "categories": "", "tags": "AngularJS", "date": "2015-02-02 05:18:47 +0500", "snippet": "There are tons for Yeoman generators for Angular JS. Some of them are really good, some of them completely missing thepoint. One of the generators is Generator Angular which was love at firstsight but after using this on a large front-end application, I found it problematic on its applicationstructure. So I decided to pull up a generator for Angular with how I would want my application structure to be. Not allare my ideas, I got a good chunk of guidelines from John Papa’s AngularJS Style Guide and his course on AngularJS Patterns: Clean Code on PluralSight.So I wired up a generator, ng-Super. I compiled all this goodness in Yeoman because I feel Yeoman makes the workflow very easy and intuitive.Here is my attempt to explain the philosophies behind ng-Super###Folder Structure####Root├── app ├── bower.json ├── configLoader.js ├── gruntfile.js├── node_modules├── package.json├── tasks └── tests The root structure is pretty standard except for the tasks folder which contains each grunt task as a separate file as shown below:tasks├── bump.js├── clean.js├── compass.js├── concurrent.js├── connect.js├── copy.js├── html2js.js├── karma.js├── ngAnnotate.js├── replace.js├── usemin.js├── useminPrepare.js├── watch.js└── wiredep.jsThis keeps maintenance of Grunt configurations very easy. Instead of managing one long/big Gruntfile, we can easily manage each configuration in a separate file.####app/src├── app.module.js├── common│ └── common.module.js├── core│ ├── core.module.js│ ├── restangularConfig.js│ └── routerConfig.js└── welcome ├── WelcomeCtrl.js ├── welcome.html └── welcome.module.jsApplication source is divided by-feature instead of by-type. The directory structure is designed to follow LIFT which is: L – Locatable files I – Identifiable at a glance F – Flat directory structure T – Try to stay DRYEach module is self contained and is designed with a single responsibility.core is the core application code, e.g. HTTP Interceptors, global router configurations. It also depends on all the other modules of the application, so this is how the application is glued up. core is available to all other modules of the application.common contains the common functionality of the application e.g. services like calendar or widgets in directives. This is the cross-cutting layer and like core, common is available to all other modules of the application.welcome is a feature module and contains all the relevant code that needs to be inside this feature including its controllers, factories, views and even routes. Each new module is added as a separate folder similar to welcome.###Styling├── css│ └── main.css├── main.scss└── partials ├── _skin.scss └── _welcome.scssStyles are done through SCSS as it provides numerous benefits on top of vanilla CSS. Details of all benefits might not be relevant here but one feature i.e. breaking up SCSS into separate files is worth a mention. Each file in the partials folder contains styling for a single purpose. This keeps our SCSS from getting messier and makes it easier to scale and maintain.###Third Party Components Grunt Restangular Twitter Bootstrap Angular UI Router Angular UI Bootstrap Lodash###Coding style####Controller(function(){ 'use strict'; angular.module('app.welcome') .controller('WelcomeCtrl', WelcomeCtrl); /* @ngInject */ function WelcomeCtrl() { var vm = this; vm.welcomeMessage = 'ZE GENGO !'; }}());Controllers use the Controller-As syntax which in real is just syntactic sugar but helps avoid scope soup and unnecessary usage of watches, event listeners and emitters. It also keeps things very clean.####Factory(function(){ 'use strict'; angular .module('app.welcome') .factory('messages', messages) /* @ngInject */ function messages(){ var service = { testFunction: testFunction } return service; //////////////////// function testFunction () { console.info('This is a test function'); } }}());Factory is designed in similar fashion. With the service object defined above and constituent functions defined below as function expressions, help identify what functions are exposed by this factory at a glance.Other parts of the application are similarly designed.###Grunt tasks####$ grunt serverPops up a development instance of the Angular JS application. It is configured with Livereload of HTML, CSS and Javascript to speed up the development flow. It concurrently runs compilation of CSS from SCSS.####$ grunt bumpBump application version and goodies, details at grunt-bump####$ grunt wiredepIterates over all Bower dependencies (CSS and JS) and add them to the index.html file####$ grunt testRuns all tests in the /tests directory with Karma and PhantomJS####$ grunt buildProduces a distributable AngularJS web application folder. The final distributable has Minified and concatenated CSS Converts all HTML templates to Javascript for speedier template serving All AngularJS source is annotated, concatenated with templates and minified to produce a single, very tiny sized source fileI think you should try it out and let me know if you have any suggestions.~Cheers" }, { "title": "Smart way of testing Angular JS filters", "url": "/posts/smart-way-of-testing-angular-js-filters/", "categories": "", "tags": "AngularJS", "date": "2014-08-16 10:22:03 +0500", "snippet": "Writing clean, scale-able and maintainable unit tests is as important as writing application source code with all those qualities. Pragmatic Programmer (yes, the famous book) says that developers are constantly in maintenance mode due to various reasons which means that the tests we write need to be updated constantly. One might think, why so much of overhead? While unit tests may contain tons of other advantages, to me, unit tests give me the confidence I need for refactoring.I want to go over a small example of how one can make its unit tests better in context of testing Angular JS custom filters. Filters usually take in a bunch of arguments and return a Javascript object (primitive or list). So technically, it has few arguments and an output. Lets look at a small custom filter I made for client side pagination with UI-Bootstrap’s Pagination Directive.angular.module(\"kuangular\").filter('kuPagination', function () { return function (list, currentPage, recordsPerPage) { if (angular.isUndefined(list) || list.length &lt;= 0) throw (\"List either undefined or empty\"); if (angular.isUndefined(currentPage) || angular.isUndefined(recordsPerPage)) throw (\"Parameters for filter are not defined. [Param 1: current page, Param 2: records per page]\"); currentPage = currentPage - 1; var startSelectionIndex, endSelectionIndex; startSelectionIndex = currentPage * recordsPerPage; endSelectionIndex = startSelectionIndex + recordsPerPage; return list.slice(startSelectionIndex, endSelectionIndex); };})Simple isn’t it ?Anyways, here is the test for the filter …describe('Filter Pagination', function () { var paginationFilter, usersList; beforeEach(module('kuangular')); beforeEach(module('Mocks')); //contains the mock objects beforeEach(inject(function ($filter, UserManagementObjects) { paginationFilter = $filter('kuPagination'); usersList = UserManagementObjects.UsersList; })); it('should throw an error if list is empty', function () { var filterfunc = function () { paginationFilter([]); } expect(filterfunc).toThrow(\"List either undefined or empty\"); }); it('should throw an error if list is undefined', function () { var filterfunc = function () { paginationFilter(undefined); } expect(filterfunc).toThrow(\"List either undefined or empty\"); }); it('should throw an error if list is defined but current page and/or records per page are undefined', function () { var filterFunc = { recordsPerPageUndefined: function () { paginationFilter(usersList, 1, undefined); }, currentPageUndefined: function () { paginationFilter(usersList, undefined, 5); }, bothUndefined: function () { paginationFilter(usersList, undefined, undefined); } } expect(filterFunc.recordsPerPageUndefined).toThrow(\"Parameters for filter are not defined. [Param 1: current page, Param 2: records per page]\"); expect(filterFunc.currentPageUndefined).toThrow(\"Parameters for filter are not defined. [Param 1: current page, Param 2: records per page]\"); expect(filterFunc.bothUndefined).toThrow(\"Parameters for filter are not defined. [Param 1: current page, Param 2: records per page]\"); }); it('should return a sliced list according to current page and records per page', function () { var retVal, testVal = []; retVal = paginationFilter(usersList, 1, 5); expect(retVal).toEqual(usersList); testVal = []; retVal = paginationFilter(usersList, 2, 2); testVal.push(usersList[2], usersList[3]); expect(retVal).toEqual(testVal); testVal = []; retVal = paginationFilter(usersList, 3, 2); testVal.push(usersList[4]); expect(retVal).toEqual(testVal); })});I did not feel comfortable at all with the quality of unit tests. It violates DRY principle a lot. It would require more effort to add/delete/update tests and if the API changes, it would require the developer to update all the unit tests with the updated API. To resolve the above issues I came up with an alternative way of testing the filter …describe('Filter Pagination', function () { var paginationFilter, usersList; beforeEach(module('kuangular')); beforeEach(module('Mocks')); beforeEach(inject(function ($filter, UserManagementObjects) { paginationFilter = $filter('kuPagination'); usersList = UserManagementObjects.UsersList; })); describe('Exception Specs', function(){ var exceptionSpecs = [ { input:[usersList, 1, undefined], output:\"Parameters for filter are not defined. [Param 1: current page, Param 2: records per page]\" }, { input:[usersList, undefined, 5], output:\"Parameters for filter are not defined. [Param 1: current page, Param 2: records per page]\" }, { input:[usersList, undefined, undefined], output:\"Parameters for filter are not defined. [Param 1: current page, Param 2: records per page]\" }, { input:[undefined], output:\"List either undefined or empty\" }, { input:[], output:\"List either undefined or empty\" }, ] it('should throw an error if list is defined but current page and/or records per page are undefined', function () { function paginationFilterWrapper (input) { paginationFilter.apply(this, input); } angular.forEach(exceptionSpecs, function(spec){ expect(paginationFilterWrapper.bind(this, spec.input)).toThrow(spec.output); }); }); }); describe('List value specs', function(){ var listSpecs = [ { input: [usersList,1,5], output:usersList }, { input: [usersList,2,2], output:Array.prototype.concat(usersList[2], usersList[3]) }, { input: [usersList, 3, 2], output:[usersList[4]] } ] it('should return a sliced list according to current page and records per page', function () { angular.forEach(listSpecs, function(spec){ expect(paginationFilter.apply(this, spec.input)).toEqual(spec.output); }); }); });});Each describe block in the above tests is responsible for a type of unit tests, i.e. exceptions and normal tests. Each contains a collection of specs that are fed to a routine inside the “it” block (which defines the test). These  tests are more maintainable and reduce the number of “it” blocks to exactly one. Add a spec on the list to add another test, pretty darn easy. Moreover it is pretty adaptable to the API changes as well and can surely make the lives of the developers much easier." }, { "title": "Simple questions that helped us deliver 2 weeks before deadline", "url": "/posts/simple-questions-that-helped-us-deliver-2-weeks-before-deadline/", "categories": "", "tags": "Software Engineering Management", "date": "2014-06-28 10:20:59 +0500", "snippet": "Although I am in the early years of my software development career, I am often exposed to discussions where we need to make a decision on the design of the application or the database. While working for a reputed security company based in the US, we were handed a task to design a windows console application which would be scheduled through the Windows Task Scheduler. Let’s say it was supposed to do task A, B and C through some .Net Web API.The team got into the discussions on how to design this console application according to the meagre requirements sent to us through the user stories. So let me describe it in a few points:Task A is completely independent of both B and C. Task A makes an API call (API A) and saves some value on the database based on the response.Task B picks up the value stored in the database by Task A and makes a call to API B. If API B is successful, API C is called to finish up the process.Let’s assume that if any API (A, B or C) fails, the app would do NOTHING.So, again everyone sat down and we had long and hard brainstorming sessions on the application design. We assumed that the app would run every few hours making no less than 100s of API calls.In the end, we designed an application which would run on multiple threads and would cater to a lot of load or at least that is what we had in mind. Moreover there were proposals of dynamically controlling the threads based on the load and God knows what not. But it turned out a perfect example of YAGNI (you aint gonna need it) i.e. speculative generalization.While everything seems to be “oh, okay that’s a great scalable design”, one fine day, one of us questioned the product owner the simplest of all questions:1)      How often will the app run, once scheduled?2)      How many calls are we expecting to make in a single run?The answers were quite shocking.1)      Once a week2)      2 – 10 API callsFrom there on it took us no more than 2 days to accomplish what we had been trying to over complicate for more than a week. And the interesting fact, we finished 2 weeks before the estimated deadline.The reason for sharing all this is pretty obvious. As a result of studying complicated frameworks, systems and other technologies, we tend to over complicate stuff for absolutely no reason, wasting our time and effort more often than not.Also, we need to understand the misuse of the phrases like, ‘what if the requirements change in the future’, such phrases do not give us a license to over complicate a simple tasks. Let’s try to keep things as simple as they can.Finally, guys, use your heads…" }, { "title": "IIS and Grunt configuration for Angular JS application development with Windows Authentication", "url": "/posts/iis-and-grunt-configuration-for-angular-js-application-development-with-windows-authentication/", "categories": "", "tags": "AngularJS, Windows Authentication", "date": "2014-05-20 06:46:23 +0500", "snippet": "There are times when one works with a ‘not so popular’ stack of technologies. The stack which I am using for development is one of those. We have a SPA based on Angular JS which consumes .Net Web API through REST services. The icing on the cake is, it uses Windows Authentication, since it is an app designed to be used on the intranet. I had a few hiccups on setting up the development environment mainly due to:How to serve my Web API and SPA through IIS with Windows AuthenticationHow to configure livereload of html, css and javascript during development while it is all being served through IIS (I hate pressing F5 over and over again). My custom Yeoman generator is used to doing things connect-livereload way. It was hard for me to configure livereload dynamically through grunt so I had a tough time setting up a development environment which uses livereload for CSS and javascript files and at the same time use the app hosted on IIS rather then using the connect plugins.Here is how I addressed these issues:Problem 1:I had a completely separate Angular JS client directory and I had to consume a .Net Web API and serve it through IIS so it had to be a part of the same website. After several attempts of structuring, I added a virtual directory to my site on IIS and pointed it to my client directory and aliased it as ‘content’. In easier words, my website contained a folder ‘content’ which contains my client application as a shortcut. Note that the client application physically existed somewhere else. Then I used IIS’s URL Rewriting to rewrite the root of my website to ‘/content’. Similarly any hit to request any static file (html, css, png, ico, js etc) was configured to be rewritten to ‘/content’. Here is a snippet from my web.config file and the URL rewriting rule.&lt;rewrite&gt; &lt;rules&gt; &lt;rule name=\"SSL_ENABLED\" enabled=\"false\" stopProcessing=\"true\"&gt; &lt;match url=\"(.*)\" /&gt; &lt;conditions&gt; &lt;add input=\"{HTTPS}\" pattern=\"^OFF$\" /&gt; &lt;/conditions&gt; &lt;action type=\"Redirect\" url=\"https://{HTTP_HOST}/{R:1}\" appendQueryString=\"true\" redirectType=\"Permanent\" /&gt; &lt;/rule&gt; &lt;rule name=\"Rewrite root to content\" stopProcessing=\"true\"&gt; &lt;match url=\"^$\" /&gt; &lt;action type=\"Rewrite\" url=\"/content/index.html\" /&gt; &lt;/rule&gt; &lt;rule name=\"Rewrite static files to content\" stopProcessing=\"true\"&gt; &lt;match url=\"(?:\\.css|\\.js|\\.html|\\.ttf|\\.jpg|\\.png|\\.gif)$\" /&gt; &lt;action type=\"Rewrite\" url=\"content/{SCRIPT_NAME}\" /&gt; &lt;/rule&gt; &lt;/rules&gt;&lt;/rewrite&gt;Problem 2:After a lot of thinking I came up with a rather obvious solution to this problem :configure grunt-contrib-watch to watch for files and reload them on the browser through watch’s livereload configurationconfigure SCSS to CSS compilation through watch’s compass block and configure it for livereloadopen the hosted app off IIS through gruntget rid of the connect-livereload all together and place the livereload script on my index.htmlBy the above steps, I can do all the IIS page serving and authentication and still use grunt for my tasks. Do read grunt-contrib-watch to understand how it works.Here is a glimpse of what the gruntfile looks like. Note, you have to place your hosted port on the WEB_PORT constant in the gruntfile.'use strict';//place the port of your IIS hostvar WEB_PORT = 1982;var LIVERELOAD_PORT = 35729;module.exports = function (grunt) { // load all grunt tasks require('matchdep').filterDev('grunt-*').forEach(grunt.loadNpmTasks); // configurable paths var yeomanConfig = { app: 'app', dist: 'dist' }; var webConfig = { \tPORT: WEB_PORT } try { yeomanConfig.app = require('./bower.json').appPath || yeomanConfig.app; } catch (e) {} grunt.initConfig({ yeoman: yeomanConfig, webConfig: webConfig, watch: { options: { nospawn: true }, compass: { files: ['&lt;%= yeoman.app %&gt;/styles/{,*/}*.{scss,sass}'], tasks: ['compass:server'], options: { livereload: LIVERELOAD_PORT } }, livereload: { options: { livereload: LIVERELOAD_PORT }, files: [ '&lt;%= yeoman.app %&gt;/{,*/}*.html', '{.tmp,&lt;%= yeoman.app %&gt;}/styles/{,*/}*.css', '{.tmp,&lt;%= yeoman.app %&gt;}/scripts/{,*/}*.js', '&lt;%= yeoman.app %&gt;/images/{,*/}*.{png,jpg,jpeg,gif,webp,svg}' ] } }, open: { server: { url: 'http://localhost:&lt;%= webConfig.PORT %&gt;' } }, //.... //lots of other grunt tasks //.... }); grunt.registerTask('server', function (target) { if (target === 'dist') { return grunt.task.run(['build', 'open', 'connect:dist:keepalive']); } grunt.task.run([ 'clean:server', 'concurrent:server', 'open', 'watch' ]); });};Now, a simple grunt server will pop up your application hosted on IIS with all the grunt and livereload magic.Happy coding :)" }, { "title": "Directive to set a scope variable on scroll finish of a div/text area", "url": "/posts/directive-to-set-a-scope-variable-on-scroll-finish-of-a-divtext-area/", "categories": "", "tags": "AngularJS", "date": "2014-03-27 05:18:47 +0500", "snippet": "I came across a problem today in which I had to enable some options (which are disabled on page load) once the text area/div which has a scroll attached to it has reached the end. Usually such are the requirements when you want the user to scroll through or read the complete text before he can continue with whatever hes doing. Since it might become a requirement for other projects as well, I decided to wrap it up inside a small and neat directive. Here is how to use it:By setting a variable, I mean setting it to a value true.There are 2 simple steps to accomplish setting a variable from the directives parent scope once the scroll has reached its end:1) Place a uk-scroll directive on the textarea/div. This would place a scroll listener on the tag with the required logic.2) Provide the uk-scroll directive with a variable on the scope to set via the set-on-scroll-complete attribute.Once the scroll has reached its end, the value of the variable on the scope, which is in the above case, radBtn would be set i.e. radBtn = trueHere is how the directive source looks like:Hope this might come in handy. Keep the suggestions/recommendations coming in.Source code can be found here" }, { "title": "Making save password work with Angular JS", "url": "/posts/making-save-password-work-with-angular-js/", "categories": "", "tags": "AngularJS", "date": "2014-03-25 04:59:17 +0500", "snippet": "With all the power Angular JS possesses, it has succumbed to a rather simple feature, that is the save password on the browsers. If you hit the ‘Save Password’ on the browser after filling in your credentials, the browser stores the information and restores it each time you visit the log in page so you can just press log in or hit enter to sign into the application. Trying that with Angular JS fails unless you add and remove a character to any input tag and try again, here is why …Angular JS uses the ‘ng-model’ directive to bond the view to a scope variable. Every time we write on the input tag, Angular invokes its digest loop and updates all the views and models (see documentation of NgModelController) by invoking different functions etc. Whenever the browser populates the saved credentials the first time, it does not invoke the digest cycle of Angular and hence none of the values that are in the view are updated on the model. Since nothing is on the scope, the login attempt would eventually fail. As mentioned above, adding and/or removing a character invokes the digest cycle of Angular doing all the dirty checking on scope variables, eventually updates the models and login request would proceed as expected.Here are a couple of simple directives which would make auto-save work like a charmAbove HTML shows a simple login form. It highlights 2 directives:1) fp-model-update-on with an attribute submitThis directive would listen for a submit event and update the model on which it is placed2) fp-submitThis directive would fire a submit event on click of whatever element it is placed onPressing enter or clicking submit button would trigger a FormSubmitted event, each model which has fp-model-update-on directive with submit as an attribute would update itself before the login request could be made. Once the models are updated, the login request can proceed as expected.Here is how the directives looks like:Simple isn’t it? Plus you can add any number of attributes and can trigger update on models on that event. Hint: blur enter etcQuestions and suggestions are welcomeThanks" } ]
